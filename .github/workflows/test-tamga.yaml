name: ðŸ§ª Tamga Test Suite

on:
  push:
    paths:
      - "tamga/**"
      - "tests/**"
      - "pyproject.toml"
      - "requirements*.txt"
  pull_request:
    paths:
      - "tamga/**"
      - "tests/**"
      - "pyproject.toml"
      - "requirements*.txt"
  workflow_dispatch:
    inputs:
      test_type:
        description: "Test type to run"
        required: true
        default: "all"
        type: choice
        options:
          - all
          - core
          - performance
          - integration
          - colors
          - time
          - security
          - quick
      python_version:
        description: "Python version (leave empty for matrix)"
        required: false
        default: ""
        type: choice
        options:
          - ""
          - "3.8"
          - "3.9"
          - "3.10"
          - "3.11"
          - "3.12"
          - "3.13"
      os:
        description: "Operating System (leave empty for matrix)"
        required: false
        default: ""
        type: choice
        options:
          - ""
          - "ubuntu-latest"
          - "windows-latest"
          - "macos-latest"
      enable_coverage:
        description: "Enable coverage reporting"
        required: false
        default: true
        type: boolean
      verbose:
        description: "Enable verbose output"
        required: false
        default: false
        type: boolean

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Dynamic test job that handles both automatic and manual triggers
  test:
    name: ðŸ Test ${{ matrix.python-version || inputs.python_version }} on ${{ matrix.os || inputs.os }}
    runs-on: ${{ matrix.os || inputs.os || 'ubuntu-latest' }}
    timeout-minutes: 15

    strategy:
      fail-fast: false
      matrix:
        # Use manual inputs if provided, otherwise use full matrix
        python-version: ${{ fromJson(inputs.python_version && format('["{0}"]', inputs.python_version) || '["3.8", "3.9", "3.10", "3.11", "3.12", "3.13"]') }}
        os: ${{ fromJson(inputs.os && format('["{0}"]', inputs.os) || '["ubuntu-latest", "windows-latest", "macos-latest"]') }}

    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: ðŸ’¾ Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-

      - name: ðŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-asyncio pytest-benchmark
          pip install -e .

      # Core Tests
      - name: ðŸ§ª Run Core Tests
        if: inputs.test_type == 'core' || inputs.test_type == 'all' || inputs.test_type == ''
        run: |
          pytest tests/test_core.py ${{ inputs.verbose && '-vv' || '-v' }} ${{ inputs.enable_coverage && '--cov=tamga --cov-report=xml' || '' }}

      # Performance Tests
      - name: âš¡ Run Performance Tests
        if: inputs.test_type == 'performance' || inputs.test_type == 'all'
        run: |
          pytest tests/test_performance.py ${{ inputs.verbose && '-vv' || '-v' }} --benchmark-only

      # Integration Tests
      - name: ðŸ”— Run Integration Tests
        if: inputs.test_type == 'integration' || inputs.test_type == 'all'
        run: |
          pytest tests/test_integration.py ${{ inputs.verbose && '-vv' || '-v' }} ${{ inputs.enable_coverage && '--cov=tamga --cov-append --cov-report=xml' || '' }}

      # Color Tests
      - name: ðŸŽ¨ Run Color Tests
        if: inputs.test_type == 'colors'
        run: |
          pytest tests/test_colors.py ${{ inputs.verbose && '-vv' || '-v' }} ${{ inputs.enable_coverage && '--cov=tamga.utils.colors --cov-report=xml' || '' }}

      # Time Tests
      - name: ðŸ•’ Run Time Tests
        if: inputs.test_type == 'time'
        run: |
          pytest tests/test_time.py ${{ inputs.verbose && '-vv' || '-v' }} ${{ inputs.enable_coverage && '--cov=tamga.utils.time --cov-report=xml' || '' }}

      # Quick Smoke Tests
      - name: ðŸš€ Run Quick Tests
        if: inputs.test_type == 'quick'
        run: |
          pytest tests/test_core.py::TestCoreLogging::test_console_logging tests/test_core.py::TestCoreLogging::test_all_log_levels -v

      # All Tests (for automatic triggers)
      - name: ðŸ§ª Run All Tests
        if: inputs.test_type == '' && github.event_name != 'workflow_dispatch'
        run: |
          pytest tests/ -v --cov=tamga --cov-report=xml --cov-report=term-missing

      # Security Tests (only on Linux)
      - name: ðŸ”’ Run Security Tests
        if: (inputs.test_type == 'security' || inputs.test_type == 'all') && matrix.os == 'ubuntu-latest'
        run: |
          pip install bandit[toml] safety
          bandit -r tamga/ -ll
          safety check --json || true

      - name: ðŸ“Š Upload coverage to Codecov
        if: inputs.enable_coverage != false && (matrix.python-version == '3.11' || inputs.python_version == '3.11') && (matrix.os == 'ubuntu-latest' || inputs.os == 'ubuntu-latest')
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

      - name: ðŸ“ˆ Generate test report
        if: always()
        run: |
          echo "## ðŸ§ª Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Type**: ${{ inputs.test_type || 'all' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Python**: ${{ matrix.python-version || inputs.python_version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **OS**: ${{ matrix.os || inputs.os }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Coverage**: ${{ inputs.enable_coverage != false && 'Enabled' || 'Disabled' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Time**: $(date)" >> $GITHUB_STEP_SUMMARY

  # Separate performance benchmark job (only runs when specifically requested)
  performance-benchmark:
    name: âš¡ Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: inputs.test_type == 'performance' && github.event_name == 'workflow_dispatch'

    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python_version || '3.11' }}

      - name: ðŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-benchmark
          pip install -e .

      - name: âš¡ Run detailed benchmarks
        run: |
          pytest tests/test_performance.py -v --benchmark-only --benchmark-verbose --benchmark-autosave

      - name: ðŸ“Š Compare with baseline (if exists)
        run: |
          if [ -f ".benchmarks/last.json" ]; then
            pytest tests/test_performance.py --benchmark-compare=0001 --benchmark-compare-fail=min:10%
          fi

      - name: ðŸ“Š Benchmark report
        run: |
          echo "## âš¡ Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "Detailed performance metrics have been generated" >> $GITHUB_STEP_SUMMARY

  # Summary job that runs after all tests
  summary:
    name: ðŸ“Š Test Summary
    runs-on: ubuntu-latest
    needs: [test]
    if: always()

    steps:
      - name: ðŸ“Š Generate summary
        run: |
          echo "## ðŸŽ¯ Tamga Test Suite Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.test.result }}" == "success" ]; then
            echo "âœ… **All tests passed!**" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Some tests failed!**" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Type**: ${{ inputs.test_type || 'all' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Manual Run**: ${{ github.event_name == 'workflow_dispatch' && 'Yes' || 'No' }}" >> $GITHUB_STEP_SUMMARY
